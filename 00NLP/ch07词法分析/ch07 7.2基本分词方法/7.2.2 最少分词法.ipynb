{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://zeag.farbox.com/post/fen-ci-ren-wu-step-by-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['他', '说', '的', '确实', '在理']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: zeag\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    def __init__(self,s, from_node=None, dis=0):\n",
    "        self.s = s#节点字符串\n",
    "        self.from_node = from_node#字符串前驱\n",
    "        self.dis = dis#从始点到该节点的最短距离\n",
    "\n",
    "\n",
    "class Segdemo:\n",
    "    def __init__(self):\n",
    "        self.word_dic = {'#开始#':2514605,'他':2826, '说':2556, '的':54478, '的确':27,'确':27,'确实':53, '实':31, '实在':35, '在':12023, '在理':1,'理':28}\n",
    "        self.trans = {'#开始#@他':1069,'他@说':355,'说@的':43,'的@确实':1 }\n",
    "\n",
    "    def conventToWordNet(self,s):#建立词网\n",
    "        word_net = []\n",
    "        word_net.append([Node('#开始#')])\n",
    "        start = 0\n",
    "        while(start < len(s)):\n",
    "            tmp = []\n",
    "            for end in range(start,len(s)):\n",
    "                tmp_str = s[start:end+1]\n",
    "                if tmp_str in self.word_dic:\n",
    "                    tmp.append(Node(tmp_str))\n",
    "                else:\n",
    "                    break\n",
    "            word_net.append(tmp)\n",
    "            start += 1\n",
    "        word_net.append([Node(\"#结束#\")])\n",
    "        return word_net\n",
    "\n",
    "    def calcDistance(self, from_node, to):#计算节点和节点的距离\n",
    "        frequency = self.word_dic[from_node]\n",
    "        nTwoWordsFreq = self.trans.get(from_node +'@'+to,0)\n",
    "        value = -np.log(0.1 * frequency / 2.5146057e7 + 0.9 * (0.9999899602323339 * nTwoWordsFreq / frequency + 1.00e-5))\n",
    "        #这个公式是用hanlp中的公式，这个公式加了一些平滑因子，总体思路就是转移次数越多，距离越小。\n",
    "        if value < 0:\n",
    "            value = -value\n",
    "        return value\n",
    "\n",
    "    def viterbi(self, word_net):\n",
    "        word_list = []\n",
    "        for i,item in enumerate(word_net):\n",
    "            if i == len(word_net)-1:\n",
    "                break\n",
    "            for from_node in item:\n",
    "                if from_node.s == \"#开始#\":\n",
    "                    to_nodes = word_net[i+1]\n",
    "                else:\n",
    "                    to_nodes = word_net[i + len(from_node.s)]\n",
    "                for to_node in to_nodes:\n",
    "                    dis = from_node.dis + self.calcDistance(from_node.s, to_node.s)\n",
    "                    if to_node.from_node == None or to_node.dis > dis:\n",
    "                        to_node.from_node = from_node\n",
    "                        to_node.dis = dis\n",
    "        word = word_net[-1][0]\n",
    "        while(word.from_node.s != \"#开始#\"):\n",
    "            word_list.append(word.from_node.s)\n",
    "            word = word.from_node\n",
    "        return word_list[::-1]\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    s = \"他说的确实在理\"\n",
    "    demo = Segdemo()\n",
    "    word_net = demo.conventToWordNet(s)\n",
    "    word_list = demo.viterbi(word_net)\n",
    "    print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
